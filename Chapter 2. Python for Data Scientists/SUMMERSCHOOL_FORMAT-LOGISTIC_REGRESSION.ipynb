{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install RISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DATA SCIENCE\n",
    "___\n",
    "### MODELS \n",
    "### LOGISTIC REGRESSION  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Author: \n",
    "#### Teaching Date: \n",
    "#### Contents:\n",
    "- [Contents](#MATHEMATICS-AND-PROGRAMMING)\n",
    " - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CLASSIFICATION PROBLEMS\n",
    "___\n",
    "\n",
    "<p> I have a dataset which is about bank loans and whether people are likely to default or not. This is an important problem for a bank because incorrectly classifying this can lead to BIG problems. I'm working with a big digital bank to help decide whether to offer people a mortgage loan or not. Firstly let's look at the data set</p>\n",
    "\n",
    "<p> When approaching a classification problem you rarely know which model will produce the best result. It all also depends on what you're modelling for and which metric you wish to use</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in ./opt/anaconda3/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in ./opt/anaconda3/lib/python3.7/site-packages (from lightgbm) (1.18.1)\n",
      "Requirement already satisfied: scipy in ./opt/anaconda3/lib/python3.7/site-packages (from lightgbm) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in ./opt/anaconda3/lib/python3.7/site-packages (from lightgbm) (0.22.1)\n",
      "Requirement already satisfied: joblib>=0.11 in ./opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->lightgbm) (0.14.1)\n"
     ]
    }
   ],
   "source": [
    "#Let's start by importing the libraries we use \n",
    "!pip install lightgbm\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#import lightgbm as lgb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Great, now let's read in the training data set \n",
    "train_df = pd.read_csv('/Users/vaishnavi/Downloads/train 2.csv')\n",
    "#Now let's put our target as a seperate dataframe so that we can drop it from the dataframe\n",
    "Target = train_df['target']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We're starting with Logistic Regression. This is one of the simplest ways of solving a classification problem. \n",
    "##### Let's start by understanding how logistic regression actually works!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw a logistic regression curve and explain what it does. \n",
    "#How do you get a logistic regression curve from a set of data?\n",
    "   \n",
    "    ''' \n",
    "    Simple you realise it's a classification problem. This means that you are trying to  \n",
    "    solve for which category something belongs to. At this point show a bunch of pictures \n",
    "    of apples and bananas. Explain how applying a regression curve to this would not be applicable.\n",
    "    Therefore, instead what do we actually want to do? Classify apples and bananas. Key word = Classify. \n",
    "    '''\n",
    "    \n",
    "    \n",
    "#Refer back to minimising the loss function\n",
    "#Logit functions\n",
    "\n",
    "\n",
    "    '''\n",
    "    How does a logistic regression work? Let's get the foundations down.\n",
    "    Firstly, in logistic regression we are predicting if something is true or false. \n",
    "    Unlike linear regression where you have a continuous variable you are trying to analyse in logistic regression we classify a category\n",
    "    Logistic regression fits an s shaped curve to the data, the curve goes from 0 to 1\n",
    "    --Draw an S shaped curve\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    Logistic regression can work with continuous and discrete data\n",
    "    How do we fit a squiggle to logistuc regression? We use maximum liklihood\n",
    "    Maximum liklihood - the best way to fit a distribution to a dataset\n",
    "    What is liklihood: optimal value for the mean or the standard deviation for a distribution give a bunch of observed measurements\n",
    "    Explain with the normal distribution: mean and standard deviation:\n",
    "    Talk about the normal distribution and the 2 parametre that define where it is centered around and the value for sigma dininf it it's thin-boned, medium etc\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have got the target variable stored it's time to drop it from our training data set\n",
    "train_input = train_df.drop(columns=['target', 'ID_code'])\n",
    "column_names = list(train_input.columns) #always worth checking you've dropped your target variables!\n",
    "column_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(train_input, Target, test_size=0.7, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (60000, 200)\n",
      "Test: (140000, 200)\n"
     ]
    }
   ],
   "source": [
    "#Quick check: Did the train_test_split work right and did we get a 70:30 split? \n",
    "print('Train:', X_train.shape)\n",
    "print('Test:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vaishnavi/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's try out Logistic Regression \n",
    "\n",
    "Model_Logistic = LogisticRegression(C=0.01, class_weight='balanced')\n",
    "Model_Logistic.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Pred = Model_Logistic.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73770604, 0.04382879, 0.4752321 , ..., 0.92282575, 0.1090069 ,\n",
       "       0.631518  ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_Logistic_Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We're going to run this on a number of models so let's get a function that can measure accuracy properly\n",
    "\n",
    "def performance(Y_test, Model_Pred):\n",
    "    logist_pred_var = [0 if i < 0.5 else 1 for i in Model_Pred]\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(Y_test, logist_pred_var))\n",
    "    fpr, tpr, thresholds = roc_curve(Y_test,Model_Pred, pos_label=1)\n",
    "    print('AUC:')\n",
    "    print(auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[97503 28378]\n",
      " [ 3450 10669]]\n",
      "AUC:\n",
      "0.8456088784216123\n"
     ]
    }
   ],
   "source": [
    "performance(Y_test, Model_Logistic_Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'format'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a680cae40a22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hi'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'format'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
